# Sport Activities Reward - Data Lakehouse

Ce projet est un **Proof of Concept (POC)** d'une infrastructure Data Engineering moderne ("Lakehouse"). Il automatise la collecte, le traitement et l'analyse des activitÃ©s sportives des employÃ©s pour calculer des primes RH et des avantages bien-Ãªtre.

-----

## Vue d'ensemble du projet

L'objectif est de consolider des donnÃ©es RH (salariÃ©s) et des donnÃ©es d'activitÃ©s sportives (type Strava) pour rÃ©pondre Ã  trois besoins mÃ©tiers :

1.  **Prime Sportive** : Calculer l'impact financier d'une prime de 5% du salaire brut pour les employÃ©s venant au travail en mobilitÃ© douce (sous condition de distance).
2.  **Jours Bien-ÃŠtre** : Identifier les employÃ©s Ã©ligibles Ã  5 jours de congÃ©s supplÃ©mentaires (ceux ayant rÃ©alisÃ© au moins 15 activitÃ©s sportives dans l'annÃ©e).
3.  **Engagement SalariÃ©** : Notifier en temps rÃ©el sur Slack les nouvelles performances sportives.

### Architecture MÃ©daillon (Bronze / Silver / Gold)

L'architecture est optimisÃ©e pour le **Temps RÃ©el** avec un traitement en cascade (Waterfall) :

  * **Ingestion & Transformation (Bronze â†’ Silver â†’ Gold)** : Un **Master Stream** unique gÃ¨re la capture des changements (CDC), le nettoyage, l'enrichissement (API Google Maps) et les agrÃ©gations mÃ©tiers en continu.
  * **Serving** : Les donnÃ©es sont immÃ©diatement disponibles pour le reporting.

-----

## Architecture Technique

Le pipeline de donnÃ©es se dÃ©compose comme suit :

1.  **Source & Simulation** :
      * **PostgreSQL** hÃ©berge les donnÃ©es opÃ©rationnelles (`rh_employees`, `sport_activities`, `strava_activities`).
      * Un script Python simule l'arrivÃ©e de nouvelles activitÃ©s sportives.
2.  **Capture & Streaming** :
      * **Debezium** capture les transactions Postgres (CDC) et les envoie dans **Redpanda** (Kafka).
      * Un **Slack Notifier** Ã©coute Kafka et envoie une alerte pour chaque nouvelle activitÃ©.
3.  **Traitement UnifiÃ© (Spark Structured Streaming)** :
      * Le script `continuous_master_stream.py` orchestre 4 flux sÃ©quentiels en boucle infinie :
        1.  **RH** : Ingestion Bronze â†’ Demande calcul distance â†’ Transformation Silver â†’ Calcul Prime (Gold).
        2.  **Sport** : Ingestion Bronze â†’ Normalisation Silver.
        3.  **Strava** : Ingestion Bronze (avec gestion de types) â†’ Transformation Silver.
        4.  **Bien-ÃŠtre** : Recalcul incrÃ©mental des Ã©ligibilitÃ©s aux congÃ©s (Gold) dÃ¨s qu'une activitÃ© arrive.
4.  **Enrichissement** :
      * Un **Distance Worker** asynchrone calcule la distance Domicile-Travail via l'API Google Maps sur demande du stream RH.
5.  **Serving** :
      * **Spark Thrift Server** expose les tables Gold via JDBC/ODBC.
      * **Power BI** consomme ces vues pour le tableau de bord final.

-----

## PrÃ©-requis

  * **Docker Desktop** installÃ© (avec au moins **8 Go de RAM** allouÃ©s, 12 Go recommandÃ©s).
  * Une clÃ© API **Google Maps** (Distance Matrix API).
  * Un Webhook **Slack** pour les notifications.

-----

## Installation et DÃ©marrage

### 1\. Configuration de l'environnement

CrÃ©ez un fichier `.env` Ã  la racine du projet avec vos clÃ©s :

```env
API_KEY_MAPS="VOTRE_CLE_GOOGLE_MAPS"
DESTINATION="1362 Av. des Platanes, 34970 Lattes"
KAFKA_BOOTSTRAP_SERVERS="redpanda:9092"
SLACK_WEBHOOK_URL="https://hooks.slack.com/services/VOTRE/WEBHOOK/ICI"
# Config BDD
DB_HOST=postgres
DB_PORT=5432
DB_NAME=rh_sport
DB_USER=postgres
DB_PASSWORD=postgres
```

### 2\. Lancement de l'infrastructure

DÃ©marrez l'ensemble des services avec Docker Compose :

```bash
docker-compose up -d --build
```

**Ce qui se passe au dÃ©marrage :**

1.  PostgreSQL s'initialise et les donnÃ©es RH initiales (CSV) sont chargÃ©es (avec validation **Pandera**).
2.  Les connecteurs Debezium sont configurÃ©s automatiquement.
3.  Le **Master Stream Processor** dÃ©marre et active la boucle de traitement (Bronze/Silver/Gold).
4.  Le **Spark Thrift Server** dÃ©marre et un script d'init crÃ©e automatiquement les vues SQL Gold pour Power BI.

-----

## Guide d'Utilisation (ScÃ©narios)

### ScÃ©nario 1 : Simulation de Vie (Nouvelles ActivitÃ©s)

Simulez l'activitÃ© des employÃ©s pour voir le pipeline rÃ©agir.

Depuis votre terminal local :

```bash
# GÃ©nÃ¨re 20 activitÃ©s alÃ©atoires
python simulate_new_strava_activities.py
```

> **RÃ©sultat ImmÃ©diat :**
>
> 1.  Notification Slack ðŸ””.
> 2.  La donnÃ©e traverse Bronze -\> Silver.
> 3.  Le compteur "ActivitÃ©s Annuelles" de l'employÃ© est mis Ã  jour dans la table Gold (visible dans les logs du stream processor).

### ScÃ©nario 2 : Visualisation Power BI

1.  Ouvrez Power BI Desktop.
2.  Connectez-vous Ã  **Spark** (`localhost:10000`), protocole **Standard**, mode **Importer**.
3.  Utilisateur : `admin` / Mdp : (vide).
4.  SÃ©lectionnez les **Vues** dans le dossier `gold` :
      * `v_prime_sportive`
      * `v_wellbeing`
5.  Cliquez sur **Actualiser** : les donnÃ©es sont Ã  jour instantanÃ©ment.

-----

## RÃ¨gles MÃ©tiers (Power BI)

Les indicateurs suivants sont prÃ©-calculÃ©s dans la couche Gold :

  * **Montant Prime Sportive** :
      * *Condition* : DÃ©placement en "Marche/Running" (\<= 15km) OU "VÃ©lo/Trottinette" (\<= 25km).
      * *Calcul* : `Salaire Brut * 5%`.
  * **Ã‰ligibilitÃ© Bien-ÃŠtre** :
      * *Condition* : Avoir rÃ©alisÃ© \>= 15 activitÃ©s dans l'annÃ©e en cours.
      * *Avantage* : 5 jours de congÃ©s.

-----

## Structure du Projet

```bash
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/          # Fichiers CSV sources
â”‚   â”œâ”€â”€ output/         # Fichiers CSV prÃªts pour ingestion
â”‚   â””â”€â”€ delta/          # STOCKAGE DATA LAKE (Bronze/Silver/Gold + Checkpoints)
â”œâ”€â”€ debezium/           # Config connecteur Postgres
â”œâ”€â”€ initdb/             # Scripts SQL (CrÃ©ation tables Postgres)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ETL_Full_Load/  # Scripts d'initialisation & Transformations
â”‚   â”œâ”€â”€ Jobs_Spark/     # Scripts Streaming & Workers
â”‚   â”‚   â”œâ”€â”€ continuous_master_stream.py  # LE CERVEAU : Orchestre tout le flux
â”‚   â”‚   â”œâ”€â”€ distance_worker.py           # Appel API Google Maps
â”‚   â”‚   â”œâ”€â”€ slack_new_activity_notifier.py # Notifications Slack
â”‚   â””â”€â”€ Dockerfile      # Image Python pour les workers
â”œâ”€â”€ spark/              # Config Spark Thrift Server
â”œâ”€â”€ simulate_new_strava_activities.py # GÃ©nÃ©rateur de donnÃ©es
â””â”€â”€ docker-compose.yml  # Orchestration globale
```

## Commandes de Maintenance

  * **Suivre le traitement temps rÃ©el :**

    ```bash
    docker logs -f stream-processor
    ```

    *(Vous verrez les logs "Batch X : Traitement de Y activitÃ©s" et "Gold Wellbeing mis Ã  jour")*

  * **VÃ©rifier les donnÃ©es brutes via SQL :**

    ```bash
    docker exec -it spark-thrift /opt/spark/bin/beeline -u "jdbc:hive2://localhost:10000" -n admin -e "SELECT COUNT(*) FROM delta.\`/data/delta/bronze/strava_activities\`;"
    ```

  * **Reset complet (Attention : supprime toutes les donnÃ©es) :**

    ```bash
    docker-compose down -v
    # Sur Linux/Mac/WSL
    rm -rf data/delta/*
    docker-compose up -d
    ```