# Sport Activities Reward - Data Lakehouse

Ce projet est un **Proof of Concept (POC)** d'une infrastructure Data Engineering moderne ("Lakehouse"). Il vise √† automatiser la collecte et l'analyse des activit√©s sportives des employ√©s pour calculer des primes et avantages RH.

## Objectifs du Projet

L'objectif est d'impl√©menter les r√®gles m√©tier suivantes √† partir de donn√©es RH et de flux d'activit√©s sportives (type Strava) :

1.  **Prime Sportive (Impact Financier)** : Accorder 5% de prime annuelle aux employ√©s venant au travail en mobilit√© douce (V√©lo, Marche, Course), sous condition de distance.
2.  **Jours Bien-√ätre** : Accorder 5 jours de cong√©s suppl√©mentaires aux employ√©s ayant r√©alis√© au moins 15 activit√©s sportives dans l'ann√©e.

## Architecture Technique

Le projet repose sur une architecture **Medallion (Bronze / Silver / Gold)** enti√®rement conteneuris√©e.

### Flux de donn√©es

1.  **Ingestion (Sources)** :
      * Donn√©es RH & R√©f√©rentiel Sport (CSV) charg√©es dans **PostgreSQL**.
      * Activit√©s sportives simul√©es (Python) inject√©es dans PostgreSQL.
2.  **Capture (CDC & Streaming)** :
      * **Debezium** capture les changements PostgreSQL en temps r√©el.
      * **Redpanda** (Kafka compatible) transmet les √©v√©nements.
3.  **Lakehouse (Spark & Delta Lake)** :
      * **Bronze Layer (Streaming)** : Ingestion brute depuis Kafka via Spark Structured Streaming (Script unifi√© `continuous_master_stream.py`).
      * **Enrichissement (Micro-batch)** : Un worker Python calcule la distance Domicile-Travail via l'API Google Maps sur demande Kafka.
      * **Silver Layer (Batch)** : Nettoyage, typage (cast Timestamp), et jointures.
      * **Gold Layer (Batch)** : Agr√©gations m√©tiers et calcul des KPIs.
4.  **Visualisation** :
      * **Spark Thrift Server** expose les tables Gold.
      * **Power BI** se connecte en JDBC/ODBC pour le reporting.

## Structure du Projet

```bash
.
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ input/          # Fichiers CSV sources (RH, Sport)
‚îÇ   ‚îú‚îÄ‚îÄ output/         # Fichiers g√©n√©r√©s (JSONL Strava, CSV transform√©s)
‚îÇ   ‚îî‚îÄ‚îÄ delta/          # STOCKAGE DATA LAKE (Bronze/Silver/Gold + Checkpoints)
‚îú‚îÄ‚îÄ debezium/           # Configuration du connecteur Postgres
‚îú‚îÄ‚îÄ initdb/             # Scripts SQL d'initialisation des tables (Schemas)
‚îú‚îÄ‚îÄ pipeline/           # (Ancien dossier, migr√© vers scripts)
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile      # Image Python pour les workers Spark/ETL
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ ETL_Full_Load/  # Scripts Batch (Init Postgres, Silver->Gold)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Postgres/   # Ingestion CSV -> DB
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Bronze/     # (Obsol√®te, remplac√© par Streaming)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Silver/     # Transformations logiques
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Gold/       # Agr√©gations finales
‚îÇ   ‚îî‚îÄ‚îÄ Jobs_Spark/     # Scripts de Streaming & Workers
‚îÇ       ‚îú‚îÄ‚îÄ continuous_master_stream.py  # MASTER JOB : Ingestion Bronze (3 flux)
‚îÇ       ‚îú‚îÄ‚îÄ batch_refresh_silver_gold.py # MASTER JOB : Propagation Silver/Gold
‚îÇ       ‚îú‚îÄ‚îÄ distance_worker.py           # Appel API Google Maps
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ spark/              # Configuration Spark Thrift Server (Dockerfile)
‚îî‚îÄ‚îÄ docker-compose.yml  # Orchestration de l'infrastructure
```

## Installation et D√©marrage

### Pr√©-requis

  * **Docker Desktop** (avec au moins 8Go de RAM allou√©s, 12Go recommand√©s).
  * **Cl√© API Google Maps** (Distance Matrix API).

### 1\. Configuration

Cr√©ez un fichier `.env` √† la racine du projet :

```env
API_KEY_MAPS=VOTRE_CLE_GOOGLE_MAPS_ICI
DESTINATION="1362 Av. des Platanes, 34970 Lattes"
KAFKA_BOOTSTRAP_SERVERS=redpanda:9092
```

### 2\. D√©marrage de l'infrastructure

Lancez l'ensemble des conteneurs (Base de donn√©es, Kafka, Spark, Workers) :

```bash
docker-compose up -d
```

*Cette commande va :*

1.  Initialiser PostgreSQL et cr√©er les tables.
2.  Charger les donn√©es CSV initiales.
3.  Lancer le **Stream Processor** qui √©coute les changements en temps r√©el.
4.  Lancer le **Spark Thrift Server** pour Power BI.
5.  Cr√©er automatiquement les Vues SQL pour Power BI une fois les donn√©es pr√™tes.

### 3\. Mise √† jour de la couche Bronze

Pour voir le pipeline r√©agir, g√©n√©rez de nouvelles activit√©s sportives :

```bash
python simulate_new_strava_activities.py
```

### 4\. Mise √† jour des couches Silver & Gold

Les couches sup√©rieures sont mises √† jour par un job Batch planifi√© (toutes les 5 min) ou manuel.
Pour forcer une mise √† jour imm√©diate :

```bash
docker-compose run --rm pipeline python scripts/Jobs_Spark/batch_refresh_silver_gold.py
```

## üìä Connexion Power BI

Pour visualiser les r√©sultats :

1.  Ouvrez **Power BI Desktop**.
2.  Cliquez sur **Obtenir les donn√©es** \> **Spark**.
3.  Configuration :
      * **Serveur** : `localhost:10000`
      * **Protocole** : `Standard` (HTTP n'est pas activ√©).
      * **Mode** : `Importer` (Recommand√©).
4.  Identifiants : `admin` / (mot de passe vide).
5.  Dans le navigateur, allez dans `spark_catalog` \> `gold`.
6.  **IMPORTANT** : S√©lectionnez les VUES (`v_prime_sportive`, `v_wellbeing`) et non les tables brutes pour √©viter les erreurs de typage.

## üõ† Commandes Utiles

**V√©rifier les donn√©es brutes (Bronze) via SQL :**

```bash
docker exec -it spark-thrift /opt/spark/bin/beeline -u "jdbc:hive2://localhost:10000" -n admin -e "SELECT COUNT(*) FROM delta.\`/data/delta/bronze/strava_activities\`;"
```

**Voir les logs du Streaming :**

```bash
docker logs -f stream-processor
```

**Red√©marrer proprement (Reset complet des donn√©es) :**

```bash
docker-compose down
# Sur Linux/Mac (ou WSL)
rm -rf data/delta/*
docker-compose up -d
```