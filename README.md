# Sport Activities Reward - Data Lakehouse

Ce projet est un **Proof of Concept (POC)** d'une infrastructure Data Engineering moderne ("Lakehouse"). Il automatise la collecte, le traitement et l'analyse des activitÃ©s sportives des employÃ©s pour calculer des primes RH et des avantages bien-Ãªtre.

-----

## Vue d'ensemble du projet

L'objectif est de consolider des donnÃ©es RH (salariÃ©s) et des donnÃ©es d'activitÃ©s sportives (type Strava) pour rÃ©pondre Ã  trois besoins mÃ©tiers :

1.  **Prime Sportive** : Calculer l'impact financier d'une prime de 5% du salaire brut pour les employÃ©s venant au travail en mobilitÃ© douce (sous condition de distance).
2.  **Jours Bien-ÃŠtre** : Identifier les employÃ©s Ã©ligibles Ã  5 jours de congÃ©s supplÃ©mentaires (ceux ayant rÃ©alisÃ© au moins 15 activitÃ©s sportives dans l'annÃ©e).
3.  **Engagement SalariÃ©** : Notifier en temps rÃ©el sur Slack les nouvelles performances sportives.

### Architecture MÃ©daillon (Bronze / Silver / Gold)

L'architecture est hybride pour optimiser les ressources :

  * **Ingestion Temps RÃ©el (Bronze)** : Capture des changements (CDC) et Ã©criture immÃ©diate dans le Data Lake.
  * **Transformation (Silver/Gold)** : Traitement par batch dÃ©clenchÃ© Ã  la demande pour recalculer les indicateurs complexes.

-----

## Architecture Technique

Le pipeline de donnÃ©es se dÃ©compose comme suit :

1.  **Source & Simulation** :
      * **PostgreSQL** hÃ©berge les donnÃ©es opÃ©rationnelles (`rh_employees`, `sport_activities`, `strava_activities`).
      * Un script Python simule l'arrivÃ©e de nouvelles activitÃ©s sportives.
2.  **Capture & Streaming** :
      * **Debezium** capture les transactions Postgres (CDC) et les envoie dans **Redpanda** (Kafka).
      * Un **Stream Processor (Spark)** Ã©coute Kafka et Ã©crit les donnÃ©es brutes dans le **Delta Lake (Couche Bronze)**.
      * Un **Slack Notifier** Ã©coute Kafka et envoie une alerte pour chaque nouvelle activitÃ©.
3.  **Enrichissement** :
      * Un **Distance Worker** calcule la distance Domicile-Travail via l'API Google Maps dÃ¨s qu'un nouvel employÃ© est dÃ©tectÃ©.
4.  **Transformation & Serving** :
      * Un script d'orchestration (`batch_refresh_silver_gold.py`) nettoie les donnÃ©es (**Silver**) et agrÃ¨ge les KPIs (**Gold**).
      * **Spark Thrift Server** expose les tables Gold via JDBC/ODBC.
      * **Power BI** consomme ces vues pour le tableau de bord final.

-----

## PrÃ©-requis

  * **Docker Desktop** installÃ© (avec au moins **8 Go de RAM** allouÃ©s, 12 Go recommandÃ©s).
  * Une clÃ© API **Google Maps** (Distance Matrix API).
  * Un Webhook **Slack** pour les notifications.

-----

## Installation et DÃ©marrage

### 1\. Configuration de l'environnement

CrÃ©ez un fichier `.env` Ã  la racine du projet avec vos clÃ©s :

```env
API_KEY_MAPS="VOTRE_CLE_GOOGLE_MAPS"
DESTINATION="1362 Av. des Platanes, 34970 Lattes"
KAFKA_BOOTSTRAP_SERVERS="redpanda:9092"
SLACK_WEBHOOK_URL="https://hooks.slack.com/services/VOTRE/WEBHOOK/ICI"
```

### 2\. Lancement de l'infrastructure

DÃ©marrez l'ensemble des services avec Docker Compose :

```bash
docker-compose up -d --build
```

**Ce qui se passe au dÃ©marrage :**

1.  PostgreSQL s'initialise et les donnÃ©es RH initiales (CSV) sont chargÃ©es.
2.  Les connecteurs Debezium sont configurÃ©s automatiquement.
3.  Le **Stream Processor** dÃ©marre et commence Ã  ingÃ©rer le Bronze en temps rÃ©el.
4.  Le **Spark Thrift Server** dÃ©marre et les vues SQL Gold sont crÃ©Ã©es automatiquement pour Power BI.

-----

## Guide d'Utilisation (ScÃ©narios)

### ScÃ©nario 1 : Simulation de Vie (Nouvelles ActivitÃ©s)

Simulez l'activitÃ© des employÃ©s pour voir le pipeline rÃ©agir.

Depuis votre terminal local :

```bash
# GÃ©nÃ¨re 20 activitÃ©s alÃ©atoires
python simulate_new_strava_activities.py
```

> **RÃ©sultat :** Vous recevrez instantanÃ©ment des notifications sur votre canal Slack ðŸ””.

### ScÃ©nario 2 : Mise Ã  jour des Dashboards (Silver & Gold)

Une fois les nouvelles donnÃ©es ingÃ©rÃ©es dans le Bronze (automatique), lancez le traitement Batch pour mettre Ã  jour les tables de reporting.

```bash
docker-compose run --rm pipeline python scripts/Jobs_Spark/batch_refresh_silver_gold.py
```

> **RÃ©sultat :** Les tables `silver.*` et `gold.*` sont mises Ã  jour avec les derniÃ¨res donnÃ©es.

### ScÃ©nario 3 : Visualisation Power BI

1.  Ouvrez Power BI Desktop.
2.  Connectez-vous Ã  **Spark** (`localhost:10000`), protocole **Standard**, mode **Importer**.
3.  Utilisateur : `admin` / Mdp : (vide).
4.  SÃ©lectionnez les **Vues** dans le dossier `gold` :
      * `v_prime_sportive`
      * `v_wellbeing`
5.  Actualisez pour voir les KPIs changer suite Ã  votre simulation.

-----

## Structure du Projet

```bash
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/          # Fichiers CSV sources
â”‚   â”œâ”€â”€ output/         # Fichiers CSV prÃªts pour ingestion dans Postgres
â”‚   â””â”€â”€ delta/          # STOCKAGE DATA LAKE (Bronze/Silver/Gold + Checkpoints)
â”œâ”€â”€ debezium/           # Config connecteur Postgres
â”œâ”€â”€ initdb/             # Scripts SQL (CrÃ©ation tables Postgres)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ETL_Full_Load/  # Scripts Batch (Init, Silver, Gold)
â”‚   â”œâ”€â”€ Jobs_Spark/     # Scripts Streaming & Workers
â”‚   â”‚   â”œâ”€â”€ continuous_master_stream.py  # Ingestion Bronze (4 flux parallÃ¨les)
â”‚   â”‚   â”œâ”€â”€ batch_refresh_silver_gold.py # Orchestrateur Batch Silver->Gold
â”‚   â”‚   â”œâ”€â”€ distance_worker.py           # Appel API Google Maps
â”‚   â”‚   â”œâ”€â”€ slack_new_activity_notifier.py # Notifications Slack
â”‚   â””â”€â”€ Dockerfile      # Image Python pour les workers
â”œâ”€â”€ spark/              # Config Spark Thrift Server
â”œâ”€â”€ simulate_new_strava_activities.py # GÃ©nÃ©rateur de donnÃ©es
â””â”€â”€ docker-compose.yml  # Orchestration globale
```

## Commandes de Maintenance

  * **VÃ©rifier les logs du streaming :** `docker logs -f stream-processor`
  * **VÃ©rifier les donnÃ©es brutes via SQL :**
    ```bash
    docker exec -it spark-thrift /opt/spark/bin/beeline -u "jdbc:hive2://localhost:10000" -n admin -e "SELECT COUNT(*) FROM delta.\`/data/delta/bronze/strava_activities\`;"
    ```
  * **Reset complet (Attention : supprime toutes les donnÃ©es) :**
    ```bash
    docker-compose down -v
    rm -rf data/delta/*
    docker-compose up -d
    ```